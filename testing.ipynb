{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\"trigram_prat.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv(\"testdata.manual.2009.06.14.csv\",header=None, names=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          date query_string      user  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009      kindle2    tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009      kindle2    vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009      kindle2    chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009      kindle2     SIX15   \n",
       "4          4   7  Mon May 11 03:21:41 UTC 2009      kindle2  yamarama   \n",
       "\n",
       "                                                text  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_label(ob):\n",
    "    if ob:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "test =df['sentiment']\n",
    "test = test.apply(fix_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493    1\n",
       "494    0\n",
       "495    1\n",
       "496    0\n",
       "497    0\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()##test is the y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df=df.drop(['id','sentiment','date','query_string','user'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "493  Ask Programming: LaTeX or InDesign?: submitted...\n",
       "494  On that note, I hate Word. I hate Pages. I hat...\n",
       "495  Ahhh... back in a *real* text editing environm...\n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
       "497  Reading the tweets coming out of Iran... The w..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loooooooovvvvvveee my kindle not that the dx is cool but the is fantastic in its own right',\n",
       " 'reading my kindle love it lee childs is good read',\n",
       " 'ok first assesment of the kindle it fucking rocks',\n",
       " 'you ll love your kindle ve had mine for few months and never looked back the new big one is huge no need for remorse',\n",
       " 'fair enough but have the kindle and think it perfect',\n",
       " 'no it is too big quite happy with the kindle',\n",
       " 'fuck this economy hate aig and their non loan given asses',\n",
       " 'jquery is my new best friend',\n",
       " 'loves twitter',\n",
       " 'how can you not love obama he makes jokes about himself',\n",
       " 'check this video out president obama at the white house correspondents dinner',\n",
       " 'firmly believe that obama pelosi have zero desire to be civil it charade and slogan but they want to destroy conservatism',\n",
       " 'house correspondents dinner was last night whoopi barbara sherri went obama got standing ovation',\n",
       " 'watchin espn jus seen this new nike commerical with puppet lebron sh was hilarious lmao',\n",
       " 'dear nike stop with the flywire that shit is waste of science and ugly love',\n",
       " 'lebron best athlete of our generation if not all time basketball related do not want to get into inter sport debates about',\n",
       " 'was talking to this guy last night and he was telling me that he is die hard spurs fan he also told me that he hates lebron james',\n",
       " 'love lebron',\n",
       " 'lebron is beast but still cheering the til the end',\n",
       " 'lebron is the boss',\n",
       " 'lebron is hometown hero to me lol love the lakers but let go cavs lol',\n",
       " 'lebron and zydrunas are such an awesome duo',\n",
       " 'lebron is beast nobody in the nba comes even close',\n",
       " 'downloading apps for my iphone so much fun there literally is an app for just about anything',\n",
       " 'good news just had call from the visa office saying everything is fine what relief am sick of scams out there stealing',\n",
       " 'awesome come back from via',\n",
       " 'in montreal for long weekend of much needed',\n",
       " 'booz allen hamilton has bad ass homegrown social collaboration platform way cool ttiv',\n",
       " 'mluc customer innovation award winner booz allen hamilton',\n",
       " 'current use the nikon and love it but not as much as the canon chose the for the video feature my mistake',\n",
       " 'need suggestions for good ir filter for my canon got some pls dm',\n",
       " 'just checked my google for my business blip shows up as the second entry huh is that good or ba',\n",
       " 'google is always good place to look should ve mentioned worked on the mustang my dad',\n",
       " 'played with an android google phone the slide out screen scares me would break that fucker so fast still prefer my iphone',\n",
       " 'us planning to resume the military tribunals at guantanamo bay only this time those on trial will be aig execs and chrysler debt holders',\n",
       " 'omg so bored my tattoooos are so itchy help aha',\n",
       " 'itchy and miserable',\n",
       " 'no not itchy for now maybe later lol',\n",
       " 'rt love the nerdy stanford human biology videos makes me miss school',\n",
       " 'has been bit crazy with steep learning curve but lyx is really good for long docs for anything shorter it would be insane',\n",
       " 'listening to by danny gokey aww he so amazing him so much',\n",
       " 'is going to sleep then on bike ride',\n",
       " 'cant sleep my tooth is aching',\n",
       " 'blah blah blah same old same old no plans today going back to sleep guess',\n",
       " 'glad didnt do bay to breakers today it freaking degrees in san francisco wtf',\n",
       " 'is in san francisco at bay to breakers',\n",
       " 'just landed at san francisco',\n",
       " 'san francisco today any suggestions',\n",
       " 'obama administration must stop bonuses to aig ponzi schemers',\n",
       " 'started to think that citi is in really deep are they gonna survive the turmoil or are they gonna be the next aig',\n",
       " 'shaunwoo hate on aig',\n",
       " 'you will not regret going to see star trek it was awesome',\n",
       " 'on my way to see star trek the esquire',\n",
       " 'going to see star trek soon with my dad',\n",
       " 'annoying new trend on the internets people picking apart michael lewis and malcolm gladwell nobody wants to read that',\n",
       " 'bill simmons in conversation with malcolm gladwell',\n",
       " 'highly recommend by malcolm gladwell',\n",
       " 'blink by malcolm gladwell amazing book and the tipping point',\n",
       " 'malcolm gladwell might be my new man crush',\n",
       " 'omg the commercials alone on espn are going to drive me nuts',\n",
       " 'playing with twitter api sounds fun may need to take class or find new friend who like to generate results with api code',\n",
       " 'playing with curl and the twitter api',\n",
       " 'hello twitter api',\n",
       " 'playing with java and the twitter api',\n",
       " 'because the twitter api is slow and most client are not good',\n",
       " 'yahoo answers can be butt sometimes',\n",
       " 'is scrapbooking with nic',\n",
       " 'rt five things wolfram alpha does better and vastly different than google',\n",
       " 'just changed my default pic to nike basketball cause bball is awesome',\n",
       " 'nike owns nba playoffs ads lebron kobe carmelo adidas billups howard marketing branding',\n",
       " 'next time ll call myself nike',\n",
       " 'new blog post nike sb dunk low premium white gum',\n",
       " 'rt was just told that nike layoffs started today',\n",
       " 'back when worked for nike we had one fav word just do it',\n",
       " 'by the way totally inspired by this freaky nike commercial',\n",
       " 'giving weka an app engine interface using the bird strike data for the tests the logo is given',\n",
       " 'brand new canon eos mp dslr camera canon mm is lens web technology thread brand new canon eos',\n",
       " 'class the is supposed to come today',\n",
       " 'needs someone to explain lambda calculus to him',\n",
       " 'took the graduate field exam for computer science today nothing makes you feel like more of an idiot than lambda calculus',\n",
       " 'shout outs to all east palo alto for being in the buildin karizmakaze cal gta also thanks to profits of doom universal hempz cracka',\n",
       " 'yeahhhhhhhhh would not really have lived in east palo alto if could have avoided it guess it only for the summer',\n",
       " 'great stanford course thanks for making it available to the public really helpful and informative for starting off',\n",
       " 'nvidia names stanford bill dally chief scientist vp of research',\n",
       " 'new blog post harvard versus stanford who wins',\n",
       " 'work til pm lets go lakers',\n",
       " 'damn you north korea',\n",
       " 'can we just go ahead and blow north korea off the map already',\n",
       " 'north korea please cease this douchebaggery china does not even like you anymore',\n",
       " 'why the hell is pelosi in freakin china and on whose dime',\n",
       " 'are you burning more cash than chrysler and gm stop the financial tsunami where bailout means taking handout',\n",
       " 'insects have infected my spinach plant',\n",
       " 'wish could catch every mosquito in the world burn em slowly they been bitin the shit outta me day mosquitos are the assholes of insects',\n",
       " 'just got back from church and totally hate insects',\n",
       " 'just got mcdonalds goddam those eggs make me sick yeah laker up date go lakers not much of an update well it true so suck it',\n",
       " 'omgg ohhdee want mcdonalds damn wonder if its open lol',\n",
       " 'history exam studying ugh',\n",
       " 'hate revision it so boring am totally unprepared for my exam tomorrow things are not looking good',\n",
       " 'higher physics exam tommorow not lookin forward to it much',\n",
       " 'it bank holiday yet only out of work now exam season sucks']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "testing = df.text[:100]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner_updated(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(my_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
      " 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
      " 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
      " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "5      1\n",
      "6      0\n",
      "7      1\n",
      "8      1\n",
      "9      1\n",
      "10     1\n",
      "11     0\n",
      "12     1\n",
      "13     1\n",
      "14     0\n",
      "15     1\n",
      "16     0\n",
      "17     1\n",
      "18     0\n",
      "19     1\n",
      "20     1\n",
      "21     1\n",
      "22     1\n",
      "23     1\n",
      "24     1\n",
      "25     1\n",
      "26     1\n",
      "27     1\n",
      "28     1\n",
      "29     1\n",
      "      ..\n",
      "468    0\n",
      "469    1\n",
      "470    1\n",
      "471    1\n",
      "472    1\n",
      "473    1\n",
      "474    0\n",
      "475    1\n",
      "476    1\n",
      "477    1\n",
      "478    1\n",
      "479    1\n",
      "480    0\n",
      "481    0\n",
      "482    0\n",
      "483    1\n",
      "484    0\n",
      "485    0\n",
      "486    1\n",
      "487    1\n",
      "488    1\n",
      "489    1\n",
      "490    0\n",
      "491    1\n",
      "492    1\n",
      "493    1\n",
      "494    0\n",
      "495    1\n",
      "496    0\n",
      "497    0\n",
      "Name: sentiment, Length: 498, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "accuracy = accuracy_score(test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.76      0.75       177\n",
      "          1       0.87      0.86      0.86       321\n",
      "\n",
      "avg / total       0.82      0.82      0.82       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = classification_report(test,y_pred)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 82.33%\n"
     ]
    }
   ],
   "source": [
    "print (\"accuracy score: {0:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['sentiment','id','date','query_string','user','text']\n",
    "# df = pd.read_csv(\"testing.csv\",encoding=\"utf-8\",header=None, names=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I love Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Hate ypu dude!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary  Chill, No need for remorse! :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Mon May 11 03:22:00 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>GeorgeVHulme</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Mon May 11 03:22:30 UTC 2009</td>\n",
       "      <td>aig</td>\n",
       "      <td>Seth937</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>Mon May 11 03:26:10 UTC 2009</td>\n",
       "      <td>jquery</td>\n",
       "      <td>dcostalis</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>Mon May 11 03:27:15 UTC 2009</td>\n",
       "      <td>twitter</td>\n",
       "      <td>PJ_King</td>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>Mon May 11 03:29:20 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>mandanicole</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          date query_string          user  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009      kindle2        tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009      kindle2        vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009      kindle2        chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009      kindle2         SIX15   \n",
       "4          4   7  Mon May 11 03:21:41 UTC 2009      kindle2      yamarama   \n",
       "5          4   8  Mon May 11 03:22:00 UTC 2009      kindle2  GeorgeVHulme   \n",
       "6          0   9  Mon May 11 03:22:30 UTC 2009          aig       Seth937   \n",
       "7          4  10  Mon May 11 03:26:10 UTC 2009       jquery     dcostalis   \n",
       "8          4  11  Mon May 11 03:27:15 UTC 2009      twitter       PJ_King   \n",
       "9          4  12  Mon May 11 03:29:20 UTC 2009        obama   mandanicole   \n",
       "\n",
       "                                                text  \n",
       "0                   @stellargirl I love Taylor Swift  \n",
       "1                                    Hate ypu dude!!  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3        @kenburbary  Chill, No need for remorse! :)  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  \n",
       "5  @richardebaker no. it is too big. I'm quite ha...  \n",
       "6  Fuck this economy. I hate aig and their non lo...  \n",
       "7                      Jquery is my new best friend.  \n",
       "8                                      Loves twitter  \n",
       "9  how can you not love Obama? he makes jokes abo...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_df=df.drop(['id','sentiment','date','query_string','user'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@stellargirl I love Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hate ypu dude!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@kenburbary  Chill, No need for remorse! :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                   @stellargirl I love Taylor Swift\n",
       "1                                    Hate ypu dude!!\n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3        @kenburbary  Chill, No need for remorse! :)\n",
       "4  @mikefish  Fair enough. But i have the Kindle2...\n",
       "5  @richardebaker no. it is too big. I'm quite ha...\n",
       "6  Fuck this economy. I hate aig and their non lo...\n",
       "7                      Jquery is my new best friend.\n",
       "8                                      Loves twitter\n",
       "9  how can you not love Obama? he makes jokes abo..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love taylor swift',\n",
       " 'hate ypu dude',\n",
       " 'ok first assesment of the kindle it fucking rocks',\n",
       " 'chill no need for remorse',\n",
       " 'fair enough but have the kindle and think it perfect',\n",
       " 'no it is too big quite happy with the kindle',\n",
       " 'fuck this economy hate aig and their non loan given asses',\n",
       " 'jquery is my new best friend',\n",
       " 'loves twitter',\n",
       " 'how can you not love obama he makes jokes about himself']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd  \n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('fivethirtyeight')\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# tok = WordPunctTokenizer()\n",
    "\n",
    "# pat1 = r'@[A-Za-z0-9_]+'\n",
    "# pat2 = r'https?://[^ ]+'\n",
    "# combined_pat = r'|'.join((pat1, pat2))\n",
    "# www_pat = r'www.[^ ]+'\n",
    "# negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "#                 \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "#                 \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "#                 \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "#                 \"mustn't\":\"must not\"}\n",
    "# neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "# def tweet_cleaner_updated(text):\n",
    "#     soup = BeautifulSoup(text, 'lxml')\n",
    "#     souped = soup.get_text()\n",
    "#     try:\n",
    "#         bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "#     except:\n",
    "#         bom_removed = souped\n",
    "#     stripped = re.sub(combined_pat, '', bom_removed)\n",
    "#     stripped = re.sub(www_pat, '', stripped)\n",
    "#     lower_case = stripped.lower()\n",
    "#     neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "#     # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "#     # I will tokenize and join together to remove unneccessary white spaces\n",
    "#     words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "#     return (\" \".join(words)).strip()\n",
    "\n",
    "# testing = df.text[:100]\n",
    "# test_result = []\n",
    "# for t in testing:\n",
    "#     test_result.append(tweet_cleaner_updated(t))\n",
    "# test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prateek= loaded_model.predict(my_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# print(y_pred_prateek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HubEnv",
   "language": "python",
   "name": "hubenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
